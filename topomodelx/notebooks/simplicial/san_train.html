
<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Train a Simplicial Attention Network (SAN) &#8212; TopoModelX latest documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script src="../../_static/documentation_options.js?v=c6e86fd7"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/simplicial/san_train';</script>
    <link rel="canonical" href="pyt-team.github.io/notebooks/simplicial/san_train.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)" href="sca_cmps_train.html" />
    <link rel="prev" title="Train a Simplicial High-Skip Network (HSN)" href="hsn_train.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Nov 9, 2023, 2:41:17‚ÄØPM"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">TopoModelX latest documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../api/index.html">
                        API Reference
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../contributing/index.html">
                        Contributing
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../../tutorials/index.html">
                        Tutorials
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../challenge/index.html">
                        ICML 2023 Topological Deep Learning Challenge
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary" tabindex="0">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../api/index.html">
                        API Reference
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../contributing/index.html">
                        Contributing
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../../tutorials/index.html">
                        Tutorials
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../challenge/index.html">
                        ICML 2023 Topological Deep Learning Challenge
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../cell/can_train.html">Train a Cell Attention Network (CAN)</a></li>




<li class="toctree-l1"><a class="reference internal" href="../cell/ccxn_train.html">Train a Convolutional Cell Complex Network (CCXN)</a></li>





<li class="toctree-l1"><a class="reference internal" href="../cell/cwn_train.html">Train a CW Network (CWN)</a></li>




</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../hypergraph/allset_train.html">Train an All-Set TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="../hypergraph/allset_transformer_train.html">Train an All-Set-Transformer TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="../hypergraph/dhgcn_train.html">Train a DHGCN TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="../hypergraph/hmpnn_train.html">Train a Hypergraph Message Passing Neural Network (HMPNN)</a></li>


<li class="toctree-l1"><a class="reference internal" href="../hypergraph/hnhn_train.html">Train a Hypergraph Networks with Hyperedge Neurons (HNHN)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../hypergraph/hnhn_train_bis.html">Train a Hypergraph Network with Hyperedge Neurons (HNHN)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../hypergraph/hypergat_train.html">Train a Hypergraph Neural Network</a></li>


<li class="toctree-l1"><a class="reference internal" href="../hypergraph/hypersage_train.html">Train a Hypersage TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="../hypergraph/unigcn_train.html">Train a UNIGCN TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="../hypergraph/unigcnii_train.html">Train a hypergraph neural network using UniGCNII layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hypergraph/unigin_train.html">Train a UNIGIN TNN</a></li>



<li class="toctree-l1"><a class="reference internal" href="../hypergraph/unisage_train.html">Train a Uni-sage TNN</a></li>



</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dist2cycle_train.html">Train a Simplicial Neural Network for Homology Localization (Dist2Cycle)</a></li>




<li class="toctree-l1"><a class="reference internal" href="hsn_train.html">Train a Simplicial High-Skip Network (HSN)</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#">Train a Simplicial Attention Network (SAN)</a></li>



<li class="toctree-l1"><a class="reference internal" href="sca_cmps_train.html">Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)</a></li>




<li class="toctree-l1"><a class="reference internal" href="sccn_train.html">Train a Simplicial Complex Convolutional Network (SCCN)</a></li>



<li class="toctree-l1"><a class="reference internal" href="sccnn_train.html">Train a SCCNN</a></li>





<li class="toctree-l1"><a class="reference internal" href="scconv_train.html">Train a Simplicial 2-complex convolutional neural network (SCConv)</a></li>



<li class="toctree-l1"><a class="reference internal" href="scn2_train.html">Train a Simplex Convolutional Network (SCN) of Rank 2</a></li>


<li class="toctree-l1"><a class="reference internal" href="scnn_train.html">Train a Simplicial Convolutional Neural Network (SCNN)</a></li>






<li class="toctree-l1"><a class="reference internal" href="scone_train.html">Train a Simplicial Complex Net (SCoNe)</a></li>






</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../tutorials/index.html" class="nav-link">Tutorials</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Train a...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="Train-a-Simplicial-Attention-Network-(SAN)">
<h1>Train a Simplicial Attention Network (SAN)<a class="headerlink" href="#Train-a-Simplicial-Attention-Network-(SAN)" title="Link to this heading">#</a></h1>
<p>We create and train a Simplicial Attention Neural Networks (SAN) originally proposed in <a class="reference external" href="https://arxiv.org/abs/2203.07485">Giusti, Battiloro et. al : Simplicial Attention Neural Networks (2022)</a>. The aim of this notebook is to be didactic and clear, for further technical and implementation details please refer to the original paper and the TopoModelX documentation.</p>
<section id="Abstract">
<h2>Abstract<a class="headerlink" href="#Abstract" title="Link to this heading">#</a></h2>
<p>The aim of this work is to introduce simplicial attention networks (SANs), i.e., novel neural architectures that operate on data defined on simplicial complexes leveraging masked self-attentional layers. Hinging on formal arguments from topological signal processing, we introduce a proper self-attention mechanism able to process data components at different layers (e.g., nodes, edges, triangles, and so on), while learning how to weight both upper and lower neighborhoods of the given topological
domain in a totally task-oriented fashion. The proposed SANs generalize most of the current architectures available for processing data defined on simplicial complexes.</p>
<center><p><img alt="SAN-architecture" src="https://i.ibb.co/PTwwDMp/SAN-architecture.jpg" /></p>
</center><p><strong>Remark.</strong> The notation we use is defined in <a class="reference external" href="https://arxiv.org/abs/2304.10031">Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)</a>and <a class="reference external" href="https://arxiv.org/pdf/2206.00606.pdf">Hajij et al : Topological Deep Learning: Going Beyond Graph Data(2023)</a>. Custom symbols are introduced along the notebook, when necessary.</p>
</section>
<section id="The-Neural-Network">
<h2>The Neural Network<a class="headerlink" href="#The-Neural-Network" title="Link to this heading">#</a></h2>
<p>The SAN layer takes rank-<span class="math notranslate nohighlight">\(r\)</span> signals as input and gives rank-<span class="math notranslate nohighlight">\(r\)</span> signals as output. The involved neighborhoods are:</p>
<p><span class="math">\begin{equation}
\mathcal N = \{\mathcal N_1, \mathcal N_2,...,\mathcal N_{2p+1}\} =  \{A_{\uparrow, r}, A_{\downarrow, r}, A_{\uparrow, r}^2, A_{\downarrow, r}^2,...,A_{\uparrow, r}^p, A_{\downarrow, r}^p, Q_r\},
\end{equation}</span> where <span class="math notranslate nohighlight">\(Q_r\)</span> is a sparse projection operator (weighted matrix) over the kernel of the <span class="math notranslate nohighlight">\(r\)</span>-th Hodge Laplacian <span class="math notranslate nohighlight">\(L_r\)</span>, computed as in the original paper. <span class="math notranslate nohighlight">\(Q_r\)</span> has the same topology of <span class="math notranslate nohighlight">\(L_r\)</span>.</p>
<p>The equation of the SAN layer of this neural network is given by:</p>
<p><span class="math">\begin{equation}
\textbf{h}_x^{t+1} =  \phi^l \Bigg ( \textbf{h}_x^{t}, \bigotimes_{\mathcal{N}_k\in\mathcal N}\bigoplus_{y \in \mathcal{N}_k(x)}  \widetilde{\alpha}_k(h_x^t,hy^t)\Bigg ),
\end{equation}</span></p>
<p>with <span class="math notranslate nohighlight">\(\widetilde{\alpha}_k\)</span> being either an attention function <span class="math notranslate nohighlight">\(\alpha_k\)</span> if <span class="math notranslate nohighlight">\(\mathcal{N}_k \neq Q_r\)</span> or a standard convolution term(affine transformation + weights) with weights given by the entries of <span class="math notranslate nohighlight">\(Q_r\)</span> if <span class="math notranslate nohighlight">\(\mathcal{N}_k = Q_r\)</span>.</p>
<p>Therefore, the SAN layer is made by an attentional convolution from rank-<span class="math notranslate nohighlight">\(r\)</span> cells to rank-<span class="math notranslate nohighlight">\(r\)</span> cells using an adjacency message passing scheme up to <span class="math notranslate nohighlight">\(p\)</span>-hops neighborhoods:</p>
<p><span class="math">\begin{align*}
&üü•\textrm{ Message.} &\quad m_{(y \rightarrow x),k} =&
\alpha_k(h_x^t,h_y^t) =
a_k(h_x^{t}, h_y^{t}) \cdot \psi_k^t(h_x^{t})\quad \forall \mathcal N_k \in \mathcal{N}\\
\\
&üüß \textrm{ Within-Neighborhood Aggregation.} &\quad m_{x,k}               =& \bigoplus_{y \in \mathcal{N}_k(x)}  m_{(y \rightarrow x),k}\\
\\
&üü© \textrm{ Between-Neighborhood Aggregation.} &\quad m_{x} =& \bigotimes_{\mathcal{N}_k\in\mathcal N}m_{x,k}\\
\\
&üü¶ \textrm{ Update.}&\quad h_x^{t+1}                =& \phi^{t}(h_x^t, m_{x})
\end{align*}</span></p>
</section>
<section id="The-Task:">
<h2>The Task:<a class="headerlink" href="#The-Task:" title="Link to this heading">#</a></h2>
<p>We train this model to perform a binary node classification task using KarateClub dataset. We use a <a class="reference external" href="https://arxiv.org/abs/1710.10903">‚ÄúGAT-like‚Äù attention function</a>, in which two different sets of attention weights <span class="math notranslate nohighlight">\(a_\uparrow\)</span> and <span class="math notranslate nohighlight">\(a_\downarrow\)</span> are learned for the upper neighborhoods <span class="math notranslate nohighlight">\(A_{\uparrow,1}^p\)</span> and for the lower neighborhoods <span class="math notranslate nohighlight">\(A_{\downarrow,1}^p\)</span> (<span class="math notranslate nohighlight">\(p=1,...,P\)</span>), respectively, i.e.:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\mathcal{N}_k \neq Q_r\)</span> and suppose, as an example, <span class="math notranslate nohighlight">\(\mathcal{N}_k = A_{\downarrow,1}^g\)</span>, the <span class="math notranslate nohighlight">\(g\)</span>-hops lower neighborhood: <span class="math">\begin{align}
&a_k(h_x^{t}, h_y^{t}) = (\textrm{softmax}_j(\textrm{LeakyReLU}(a_{\downarrow}^T[\underset{p=1}{\overset{P}{||}}h_x^{t}W_{\downarrow,p}|| \underset{p=1}{\overset{P}{||}}h_y^{t}W_{\downarrow,p}]))^g\\
& \psi_k^t(h_x^{t}) = h_x^{t}W_{\downarrow,g}.
\end{align}</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(\mathcal{N}_k = Q_r\)</span>: <span class="math">\begin{align}
&a_k(h_x^{t}, h_y^{t}) = Q_{x,y}\\
& \psi_k^t(h_x^{t}) = h_x^{t}W.
\end{align}</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(W\)</span>, <span class="math notranslate nohighlight">\(a_\downarrow\)</span>, <span class="math notranslate nohighlight">\(a_\uparrow\)</span>, {<span class="math notranslate nohighlight">\(W_{\downarrow,p}\}_{p=1}^P\)</span> and <span class="math notranslate nohighlight">\(\{W_{\uparrow,p}\}_{p=1}^P\)</span> are learnable weights.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">import</span> <span class="nn">toponetx.datasets.graph</span> <span class="k">as</span> <span class="nn">graph</span>
<span class="kn">from</span> <span class="nn">torch_geometric.utils.convert</span> <span class="kn">import</span> <span class="n">to_networkx</span>

<span class="kn">from</span> <span class="nn">topomodelx.nn.simplicial.san</span> <span class="kn">import</span> <span class="n">SAN</span>
<span class="kn">from</span> <span class="nn">topomodelx.utils.sparse</span> <span class="kn">import</span> <span class="n">from_sparse</span>

<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
cpu
</pre></div></div>
</div>
</section>
</section>
<section id="Pre-processing">
<h1>Pre-processing<a class="headerlink" href="#Pre-processing" title="Link to this heading">#</a></h1>
<p>The first step is to import the Karate Club (<a class="reference external" href="https://www.jstor.org/stable/3629752">https://www.jstor.org/stable/3629752</a>) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.</p>
<p>We must first lift our graph dataset into the simplicial complex domain.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">karate_club</span><span class="p">(</span><span class="n">complex_type</span><span class="o">=</span><span class="s2">&quot;simplicial&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> Out [21]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(34, 78, 45, 11, 2)
</pre></div></div>
</div>
<p>We now retrieve the neighborhoods (i.e. their representative matrices) that we will use to send messages on the domain. In this case, we decide w.l.o.g. to work at the edge level (thus considering a simplicial complex of order 2). We therefore need the lower and upper laplacians of rank 1, <span class="math notranslate nohighlight">\(L_{\downarrow,1}=B_1^TB_1\)</span> and <span class="math notranslate nohighlight">\(L_{\uparrow,1}=B_2B_2^T\)</span>, both with dimensions <span class="math notranslate nohighlight">\(n_\text{edges} \times n_\text{edges}\)</span>, where <span class="math notranslate nohighlight">\(B_1\)</span> and <span class="math notranslate nohighlight">\(B_2\)</span> are the incidence matrices of rank 1
and 2. Please notice that the binary adjacencies <span class="math notranslate nohighlight">\(A_{\downarrow,1}^p\)</span> and <span class="math notranslate nohighlight">\(A_{\uparrow,1}^p\)</span> encoding the <span class="math notranslate nohighlight">\(p\)</span>-hops neighborhoods are given by the support (the non-zeros pattern) of <span class="math notranslate nohighlight">\(L_{\downarrow,1}^p\)</span> and <span class="math notranslate nohighlight">\(L_{\uparrow,1}^p\)</span>, respectively. We also convert the neighborhood structures to torch tensors.</p>
<p><strong>Remark.</strong> In the case of rank-0 simplices (nodes), there is no lower Laplacian; in this case, we just initialize the down laplacian as a 0-matrix, and SAN automatically becomes a GAT-like architecture. In the case of simplices of maxium rank (the order of the complex), there is no upper Laplacian. In this case we can also initialize it as a 0 matrix and SAN will only consider the lower adjacencies.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simplex_order_k</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1"># Down laplacian</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">laplacian_down</span> <span class="o">=</span> <span class="n">from_sparse</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">down_laplacian_matrix</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="n">simplex_order_k</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="n">laplacian_down</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">simplex_order_k</span><span class="p">],</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">simplex_order_k</span><span class="p">])</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span>
<span class="c1"># Up laplacian</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">laplacian_up</span> <span class="o">=</span> <span class="n">from_sparse</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">up_laplacian_matrix</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="n">simplex_order_k</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
    <span class="n">laplacian_up</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">simplex_order_k</span><span class="p">],</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">simplex_order_k</span><span class="p">])</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>We define edge features to be the gradient of the nodes features, i.e. given the node feature matrix <span class="math notranslate nohighlight">\(X_0\)</span>, we compute the edge features matrix as <span class="math notranslate nohighlight">\(X_1 = B_1^TX_0\)</span>. We will finally obtain the estimated node labels from the updated edge features by multiplying them again with <span class="math notranslate nohighlight">\(B_1\)</span>, i.e. the final nodes features are computed as the divergence of the final edge features.</p>
<p><strong>Remark.</strong> Please notice that also this way of deriving edges/nodes features from nodes/edges features could be seen as a (non-learnable) message passing between rank-0/1 cells (nodes/edges) and rank-1/0 cells (nodes).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_0</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_simplex_attributes</span><span class="p">(</span><span class="s2">&quot;node_feat&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">x_0</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">x_0</span><span class="p">))</span>
<span class="n">channels_nodes</span> <span class="o">=</span> <span class="n">x_0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="n">x_0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> nodes with features of dimension </span><span class="si">{</span><span class="n">x_0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

<span class="n">x_1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_simplex_attributes</span><span class="p">(</span><span class="s2">&quot;edge_feat&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">x_1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">x_1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="n">x_1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> edges with features of dimension </span><span class="si">{</span><span class="n">x_1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

<span class="n">x_2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_simplex_attributes</span><span class="p">(</span><span class="s2">&quot;face_feat&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">x_2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">x_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="n">x_2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> faces with features of dimension </span><span class="si">{</span><span class="n">x_2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
There are 34 nodes with features of dimension 2.
There are 78 edges with features of dimension 2.
There are 45 faces with features of dimension 2.
</pre></div></div>
</div>
<p>We use the incidence matrix between nodes-edges:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">incidence_0_1</span> <span class="o">=</span> <span class="n">from_sparse</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">incidence_matrix</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>The final edge features are obtained summing the original features of those edges plus the projection of the node features onto edges (using the incidence matrix accordingly):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">incidence_0_1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">x_0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Hence, the final input features are defined by this sum, and we also pre-define the number of hidden and output channels of the model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">hidden_channels</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">out_channels</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
<p>We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.</p>
<p>We convert one-hot encode the binary labels, and keep the first four nodes for the purpose of testing.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">34</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y_true</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
<span class="n">y_true</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">[:</span><span class="mi">30</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Create-the-Neural-Network">
<h1>Create the Neural Network<a class="headerlink" href="#Create-the-Neural-Network" title="Link to this heading">#</a></h1>
<p>Using the SAN class, we create our neural network with stacked layers. Given the considered dataset and task (Karate Club, node classification), a linear layer at the end produces an output with shape <span class="math notranslate nohighlight">\(n_\text{nodes} \times 2\)</span>, so we can compare with our binary labels.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [43]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span> <span class="o">=</span> <span class="n">SAN</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">hidden_channels</span><span class="o">=</span><span class="n">hidden_channels</span><span class="p">,</span>
            <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">laplacian_up</span><span class="p">,</span> <span class="n">laplacian_down</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">laplacian_up</span><span class="p">,</span> <span class="n">laplacian_down</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">laplacian_up</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">laplacian_down</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_1</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> Out [44]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(torch.Size([78, 78]), torch.Size([78, 78]), torch.Size([78, 2]))
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [47]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_layers</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
    <span class="n">hidden_channels</span><span class="o">=</span><span class="n">hidden_channels</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Train-the-Neural-Network">
<h1>Train the Neural Network<a class="headerlink" href="#Train-the-Neural-Network" title="Link to this heading">#</a></h1>
<p>The following cell performs the training, looping over the network for a low number of epochs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [48]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_interval</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="k">for</span> <span class="n">epoch_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">epoch_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">y_hat_edge</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">laplacian_up</span><span class="o">=</span><span class="n">laplacian_up</span><span class="p">,</span> <span class="n">laplacian_down</span><span class="o">=</span><span class="n">laplacian_down</span><span class="p">)</span>
    <span class="c1"># We project the edge-level output of the model to the node-level</span>
    <span class="c1"># and apply softmax fn to get the final node-level classification output</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">incidence_0_1</span><span class="p">,</span> <span class="n">y_hat_edge</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">y_hat</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)]</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="n">epoch_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_hat</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)]</span> <span class="o">==</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch_i</span><span class="si">}</span><span class="s2"> loss: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">epoch_loss</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> Train_acc: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">epoch_i</span> <span class="o">%</span> <span class="n">test_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">y_hat_edge_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="n">laplacian_up</span><span class="o">=</span><span class="n">laplacian_up</span><span class="p">,</span> <span class="n">laplacian_down</span><span class="o">=</span><span class="n">laplacian_down</span>
            <span class="p">)</span>
            <span class="c1"># Projection to node-level</span>
            <span class="n">y_hat_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">incidence_0_1</span><span class="p">,</span> <span class="n">y_hat_edge_test</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                <span class="n">y_hat_test</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="c1"># _pred_test = torch.softmax(y_hat_test,dim=1).ge(0.5).float()</span>
            <span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">y_pred_test</span><span class="p">[</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span> <span class="p">:],</span> <span class="n">y_test</span><span class="p">)</span>
                <span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                <span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test_acc: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Epoch: 1 loss: 0.7225 Train_acc: 0.7000
Epoch: 2 loss: 0.7185 Train_acc: 0.7000
Epoch: 3 loss: 0.7139 Train_acc: 0.7000
Epoch: 4 loss: 0.7091 Train_acc: 0.7333
Epoch: 5 loss: 0.7044 Train_acc: 0.7333
Epoch: 6 loss: 0.6999 Train_acc: 0.7333
Epoch: 7 loss: 0.6958 Train_acc: 0.7333
Epoch: 8 loss: 0.6922 Train_acc: 0.7333
Epoch: 9 loss: 0.6890 Train_acc: 0.7333
Epoch: 10 loss: 0.6862 Train_acc: 0.7333
Test_acc: 0.2500
Epoch: 11 loss: 0.6837 Train_acc: 0.7333
Epoch: 12 loss: 0.6816 Train_acc: 0.7333
Epoch: 13 loss: 0.6798 Train_acc: 0.7333
Epoch: 14 loss: 0.6782 Train_acc: 0.7333
Epoch: 15 loss: 0.6768 Train_acc: 0.7333
Epoch: 16 loss: 0.6756 Train_acc: 0.7333
Epoch: 17 loss: 0.6746 Train_acc: 0.7333
Epoch: 18 loss: 0.6737 Train_acc: 0.7333
Epoch: 19 loss: 0.6730 Train_acc: 0.7333
Epoch: 20 loss: 0.6723 Train_acc: 0.7333
Test_acc: 0.2500
Epoch: 21 loss: 0.6717 Train_acc: 0.7333
Epoch: 22 loss: 0.6712 Train_acc: 0.7333
Epoch: 23 loss: 0.6708 Train_acc: 0.7333
Epoch: 24 loss: 0.6704 Train_acc: 0.7333
Epoch: 25 loss: 0.6700 Train_acc: 0.7333
Epoch: 26 loss: 0.6697 Train_acc: 0.7333
Epoch: 27 loss: 0.6694 Train_acc: 0.7333
Epoch: 28 loss: 0.6692 Train_acc: 0.7333
Epoch: 29 loss: 0.6689 Train_acc: 0.7333
Epoch: 30 loss: 0.6687 Train_acc: 0.7333
Test_acc: 0.2500
Epoch: 31 loss: 0.6685 Train_acc: 0.7333
Epoch: 32 loss: 0.6683 Train_acc: 0.7333
Epoch: 33 loss: 0.6682 Train_acc: 0.7333
Epoch: 34 loss: 0.6680 Train_acc: 0.7333
Epoch: 35 loss: 0.6678 Train_acc: 0.7333
Epoch: 36 loss: 0.6677 Train_acc: 0.7333
Epoch: 37 loss: 0.6676 Train_acc: 0.7333
Epoch: 38 loss: 0.6674 Train_acc: 0.7333
Epoch: 39 loss: 0.6673 Train_acc: 0.7333
Epoch: 40 loss: 0.6672 Train_acc: 0.7333
Test_acc: 0.2500
Epoch: 41 loss: 0.6670 Train_acc: 0.7333
Epoch: 42 loss: 0.6669 Train_acc: 0.7333
Epoch: 43 loss: 0.6668 Train_acc: 0.7333
Epoch: 44 loss: 0.6666 Train_acc: 0.7333
Epoch: 45 loss: 0.6665 Train_acc: 0.7333
Epoch: 46 loss: 0.6664 Train_acc: 0.7333
Epoch: 47 loss: 0.6663 Train_acc: 0.7333
Epoch: 48 loss: 0.6661 Train_acc: 0.7333
Epoch: 49 loss: 0.6660 Train_acc: 0.7333
Epoch: 50 loss: 0.6659 Train_acc: 0.7333
Test_acc: 0.2500
Epoch: 51 loss: 0.6657 Train_acc: 0.7333
Epoch: 52 loss: 0.6656 Train_acc: 0.7333
Epoch: 53 loss: 0.6654 Train_acc: 0.7333
Epoch: 54 loss: 0.6653 Train_acc: 0.7333
Epoch: 55 loss: 0.6652 Train_acc: 0.7333
Epoch: 56 loss: 0.6650 Train_acc: 0.7333
Epoch: 57 loss: 0.6648 Train_acc: 0.7333
Epoch: 58 loss: 0.6647 Train_acc: 0.7333
Epoch: 59 loss: 0.6645 Train_acc: 0.7333
Epoch: 60 loss: 0.6644 Train_acc: 0.7333
Test_acc: 0.2500
Epoch: 61 loss: 0.6642 Train_acc: 0.7333
Epoch: 62 loss: 0.6640 Train_acc: 0.7333
Epoch: 63 loss: 0.6638 Train_acc: 0.7333
Epoch: 64 loss: 0.6637 Train_acc: 0.7333
Epoch: 65 loss: 0.6635 Train_acc: 0.7333
Epoch: 66 loss: 0.6633 Train_acc: 0.7333
Epoch: 67 loss: 0.6631 Train_acc: 0.7333
Epoch: 68 loss: 0.6629 Train_acc: 0.7333
Epoch: 69 loss: 0.6627 Train_acc: 0.7333
Epoch: 70 loss: 0.6624 Train_acc: 0.7333
Test_acc: 0.2500
Epoch: 71 loss: 0.6622 Train_acc: 0.7333
Epoch: 72 loss: 0.6620 Train_acc: 0.7333
Epoch: 73 loss: 0.6618 Train_acc: 0.7333
Epoch: 74 loss: 0.6615 Train_acc: 0.7333
Epoch: 75 loss: 0.6613 Train_acc: 0.7333
Epoch: 76 loss: 0.6610 Train_acc: 0.7333
Epoch: 77 loss: 0.6608 Train_acc: 0.7333
Epoch: 78 loss: 0.6605 Train_acc: 0.7333
Epoch: 79 loss: 0.6603 Train_acc: 0.7333
Epoch: 80 loss: 0.6600 Train_acc: 0.7333
Test_acc: 0.2500
Epoch: 81 loss: 0.6597 Train_acc: 0.7333
Epoch: 82 loss: 0.6594 Train_acc: 0.7333
Epoch: 83 loss: 0.6592 Train_acc: 0.7333
Epoch: 84 loss: 0.6589 Train_acc: 0.7333
Epoch: 85 loss: 0.6586 Train_acc: 0.7333
Epoch: 86 loss: 0.6583 Train_acc: 0.7667
Epoch: 87 loss: 0.6580 Train_acc: 0.7667
Epoch: 88 loss: 0.6577 Train_acc: 0.7667
Epoch: 89 loss: 0.6574 Train_acc: 0.7667
Epoch: 90 loss: 0.6571 Train_acc: 0.7667
Test_acc: 0.2500
Epoch: 91 loss: 0.6568 Train_acc: 0.7667
Epoch: 92 loss: 0.6565 Train_acc: 0.7667
Epoch: 93 loss: 0.6562 Train_acc: 0.7667
Epoch: 94 loss: 0.6559 Train_acc: 0.7667
Epoch: 95 loss: 0.6556 Train_acc: 0.7667
Epoch: 96 loss: 0.6553 Train_acc: 0.8000
Epoch: 97 loss: 0.6550 Train_acc: 0.8000
Epoch: 98 loss: 0.6548 Train_acc: 0.8000
Epoch: 99 loss: 0.6545 Train_acc: 0.8000
Epoch: 100 loss: 0.6543 Train_acc: 0.8000
Test_acc: 0.2500
Epoch: 101 loss: 0.6541 Train_acc: 0.8000
Epoch: 102 loss: 0.6539 Train_acc: 0.8333
Epoch: 103 loss: 0.6537 Train_acc: 0.8333
Epoch: 104 loss: 0.6535 Train_acc: 0.8333
Epoch: 105 loss: 0.6534 Train_acc: 0.8333
Epoch: 106 loss: 0.6532 Train_acc: 0.8333
Epoch: 107 loss: 0.6531 Train_acc: 0.8333
Epoch: 108 loss: 0.6529 Train_acc: 0.8333
Epoch: 109 loss: 0.6528 Train_acc: 0.8333
Epoch: 110 loss: 0.6526 Train_acc: 0.8333
Test_acc: 0.2500
Epoch: 111 loss: 0.6525 Train_acc: 0.8333
Epoch: 112 loss: 0.6524 Train_acc: 0.8333
Epoch: 113 loss: 0.6522 Train_acc: 0.8333
Epoch: 114 loss: 0.6521 Train_acc: 0.8333
Epoch: 115 loss: 0.6520 Train_acc: 0.8333
Epoch: 116 loss: 0.6518 Train_acc: 0.8333
Epoch: 117 loss: 0.6517 Train_acc: 0.8333
Epoch: 118 loss: 0.6516 Train_acc: 0.8333
Epoch: 119 loss: 0.6515 Train_acc: 0.8333
Epoch: 120 loss: 0.6513 Train_acc: 0.8333
Test_acc: 0.2500
Epoch: 121 loss: 0.6512 Train_acc: 0.8333
Epoch: 122 loss: 0.6511 Train_acc: 0.8333
Epoch: 123 loss: 0.6510 Train_acc: 0.8333
Epoch: 124 loss: 0.6509 Train_acc: 0.8333
Epoch: 125 loss: 0.6508 Train_acc: 0.8333
Epoch: 126 loss: 0.6507 Train_acc: 0.8333
Epoch: 127 loss: 0.6506 Train_acc: 0.8333
Epoch: 128 loss: 0.6505 Train_acc: 0.8333
Epoch: 129 loss: 0.6504 Train_acc: 0.8333
Epoch: 130 loss: 0.6504 Train_acc: 0.8333
Test_acc: 0.2500
Epoch: 131 loss: 0.6503 Train_acc: 0.8333
Epoch: 132 loss: 0.6502 Train_acc: 0.8333
Epoch: 133 loss: 0.6501 Train_acc: 0.8333
Epoch: 134 loss: 0.6500 Train_acc: 0.8333
Epoch: 135 loss: 0.6500 Train_acc: 0.8333
Epoch: 136 loss: 0.6499 Train_acc: 0.8333
Epoch: 137 loss: 0.6498 Train_acc: 0.8333
Epoch: 138 loss: 0.6498 Train_acc: 0.8333
Epoch: 139 loss: 0.6497 Train_acc: 0.8333
Epoch: 140 loss: 0.6496 Train_acc: 0.8333
Test_acc: 0.2500
Epoch: 141 loss: 0.6495 Train_acc: 0.8333
Epoch: 142 loss: 0.6495 Train_acc: 0.8333
Epoch: 143 loss: 0.6494 Train_acc: 0.8333
Epoch: 144 loss: 0.6494 Train_acc: 0.8333
Epoch: 145 loss: 0.6493 Train_acc: 0.8333
Epoch: 146 loss: 0.6492 Train_acc: 0.8333
Epoch: 147 loss: 0.6492 Train_acc: 0.8333
Epoch: 148 loss: 0.6491 Train_acc: 0.8333
Epoch: 149 loss: 0.6491 Train_acc: 0.8333
Epoch: 150 loss: 0.6490 Train_acc: 0.8333
Test_acc: 0.2500
Epoch: 151 loss: 0.6490 Train_acc: 0.8333
Epoch: 152 loss: 0.6489 Train_acc: 0.8333
Epoch: 153 loss: 0.6489 Train_acc: 0.8333
Epoch: 154 loss: 0.6488 Train_acc: 0.8333
Epoch: 155 loss: 0.6488 Train_acc: 0.8333
Epoch: 156 loss: 0.6487 Train_acc: 0.8333
Epoch: 157 loss: 0.6487 Train_acc: 0.8333
Epoch: 158 loss: 0.6486 Train_acc: 0.8333
Epoch: 159 loss: 0.6486 Train_acc: 0.8333
Epoch: 160 loss: 0.6485 Train_acc: 0.8333
Test_acc: 0.2500
Epoch: 161 loss: 0.6485 Train_acc: 0.8333
Epoch: 162 loss: 0.6484 Train_acc: 0.8333
Epoch: 163 loss: 0.6484 Train_acc: 0.8333
Epoch: 164 loss: 0.6483 Train_acc: 0.8333
Epoch: 165 loss: 0.6483 Train_acc: 0.8333
Epoch: 166 loss: 0.6483 Train_acc: 0.8333
Epoch: 167 loss: 0.6482 Train_acc: 0.8333
Epoch: 168 loss: 0.6482 Train_acc: 0.8333
Epoch: 169 loss: 0.6481 Train_acc: 0.8333
Epoch: 170 loss: 0.6481 Train_acc: 0.8333
Test_acc: 0.2500
Epoch: 171 loss: 0.6481 Train_acc: 0.8333
Epoch: 172 loss: 0.6480 Train_acc: 0.8333
Epoch: 173 loss: 0.6480 Train_acc: 0.8333
Epoch: 174 loss: 0.6480 Train_acc: 0.8333
Epoch: 175 loss: 0.6479 Train_acc: 0.8333
Epoch: 176 loss: 0.6479 Train_acc: 0.8333
Epoch: 177 loss: 0.6479 Train_acc: 0.8333
Epoch: 178 loss: 0.6478 Train_acc: 0.8333
Epoch: 179 loss: 0.6478 Train_acc: 0.8333
Epoch: 180 loss: 0.6477 Train_acc: 0.8333
Test_acc: 0.2500
Epoch: 181 loss: 0.6477 Train_acc: 0.8333
Epoch: 182 loss: 0.6477 Train_acc: 0.8333
Epoch: 183 loss: 0.6477 Train_acc: 0.8333
Epoch: 184 loss: 0.6476 Train_acc: 0.8333
Epoch: 185 loss: 0.6476 Train_acc: 0.8333
Epoch: 186 loss: 0.6476 Train_acc: 0.8333
Epoch: 187 loss: 0.6475 Train_acc: 0.8333
Epoch: 188 loss: 0.6475 Train_acc: 0.8333
Epoch: 189 loss: 0.6475 Train_acc: 0.8333
Epoch: 190 loss: 0.6474 Train_acc: 0.8333
Test_acc: 0.2500
Epoch: 191 loss: 0.6474 Train_acc: 0.8333
Epoch: 192 loss: 0.6474 Train_acc: 0.8333
Epoch: 193 loss: 0.6474 Train_acc: 0.8333
Epoch: 194 loss: 0.6473 Train_acc: 0.8333
Epoch: 195 loss: 0.6473 Train_acc: 0.8333
Epoch: 196 loss: 0.6473 Train_acc: 0.8333
Epoch: 197 loss: 0.6472 Train_acc: 0.8333
Epoch: 198 loss: 0.6472 Train_acc: 0.8333
Epoch: 199 loss: 0.6472 Train_acc: 0.8333
Epoch: 200 loss: 0.6472 Train_acc: 0.8333
Test_acc: 0.2500
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span> In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="hsn_train.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Train a Simplicial High-Skip Network (HSN)</p>
      </div>
    </a>
    <a class="right-next"
       href="sca_cmps_train.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Train a Simplicial Complex Autoencoder (SCA) with Coadjacency Message Passing Scheme (CMPS)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Train a Simplicial Attention Network (SAN)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#Abstract">Abstract</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#The-Neural-Network">The Neural Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#The-Task:">The Task:</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#Pre-processing">Pre-processing</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#Create-the-Neural-Network">Create the Neural Network</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#Train-the-Neural-Network">Train the Neural Network</a></li>
</ul>

  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../../_sources/notebooks/simplicial/san_train.ipynb.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      ¬© Copyright 2022-2023, PyT-Team, Inc..
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.14.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>